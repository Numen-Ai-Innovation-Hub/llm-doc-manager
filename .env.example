# ============================================
# LLM Doc Manager - Environment Variables
# ============================================

# ============================================
# OpenAI Configuration
# ============================================
OPENAI_PROVIDER=openai
OPENAI_MODEL=gpt-4o-mini
OPENAI_BASE_URL=https://api.openai.com/v1
OPENAI_API_KEY=your-openai-api-key-here
OPENAI_TEMPERATURE=0.3
OPENAI_MAX_TOKENS=8000

# ============================================
# Anthropic Configuration (1M Context Window)
# ============================================
ANTHROPIC_PROVIDER=anthropic
ANTHROPIC_MODEL=claude-3-7-sonnet-20250219
ANTHROPIC_BASE_URL=https://api.anthropic.com
ANTHROPIC_API_KEY=your-anthropic-api-key-here
ANTHROPIC_TEMPERATURE=0.3
ANTHROPIC_MAX_TOKENS=8000

# ============================================
# Ollama Configuration (Local Models)
# ============================================
OLLAMA_PROVIDER=ollama
OLLAMA_MODEL=llama2
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_API_KEY=
OLLAMA_TEMPERATURE=0.3
OLLAMA_MAX_TOKENS=4000

# ============================================
# Usage Guide
# ============================================
# Crie uma configuração especificando o provider:
#   config = Config(llm=LLMConfig(provider="openai"))    # Usa OpenAI
#   config = Config(llm=LLMConfig(provider="anthropic")) # Usa Anthropic
#   config = Config(llm=LLMConfig(provider="ollama"))    # Usa Ollama
#
# Temperature e Max Tokens:
#   - temperature: 0.0-1.0 (menor = mais determinístico, maior = mais criativo)
#   - max_tokens: Máximo de tokens na resposta (Anthropic suporta mais tokens)